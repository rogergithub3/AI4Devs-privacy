# Protecci√≥n de Datos Entrenamiento en Inteligencia Artificial

## üõ°Ô∏è ¬øC√≥mo protege la IA los datos de entrenamiento?

| Mecanismo de Protecci√≥n            | Descripci√≥n corta                                                                 |
|----------------------------------|-----------------------------------------------------------------------------------|
| **An√°lisis y filtrado de datos**  | Se eliminan datos sensibles o personales antes del entrenamiento.                |
| **Anonimizaci√≥n**                 | Se eliminan o transforman identificadores personales para evitar reidentificaci√≥n.|
| **Differential Privacy**          | Se a√±ade "ruido" a los datos o resultados para proteger la privacidad individual. |
| **Access Control**                | Se limita el acceso a los datasets sensibles a personal autorizado.              |
| **Data Encryption**               | Los datos se cifran durante el almacenamiento y la transmisi√≥n.                 |
| **Federated Learning**           | El modelo se entrena localmente en dispositivos sin enviar datos al servidor.    |
| **Auditor√≠a y trazabilidad**     | Se registra el uso de los datos para detectar accesos indebidos.                 |

## üìä Ejemplos por t√©cnica

| T√©cnica               | Ejemplo                                                      |
|----------------------|--------------------------------------------------------------|
| Anonimizaci√≥n         | Nombre "Juan P√©rez" ‚Üí "Usuario123"                           |
| Differential Privacy | Frecuencia de enfermedades con ruido estad√≠stico a√±adido     |
| Federated Learning   | Teclado m√≥vil aprende sin subir texto al servidor            |
| Cifrado               | Dataset cifrado con AES-256 en almacenamiento cloud         |
| Acceso restringido    | Solo investigadores con autorizaci√≥n acceden a datos cl√≠nicos|

## üß† ¬øC√≥mo evitar que la IA memorice datos sensibles?

| T√©cnica                        | Descripci√≥n corta                                                                 |
|-------------------------------|-----------------------------------------------------------------------------------|
| **Regularizaci√≥n**            | Penaliza la complejidad del modelo para evitar sobreajuste y memorizaci√≥n.       |
| **Limitar tama√±o del batch**  | Entrenar con lotes peque√±os reduce la posibilidad de memorizar ejemplos exactos. |
| **Early Stopping**            | Detiene el entrenamiento antes de que el modelo memorice datos.                  |
| **Differential Privacy**      | Controla cu√°nto puede aprender el modelo de cada ejemplo individual.              |
| **Dataset Sanitization**      | Revisi√≥n manual o autom√°tica para eliminar datos sensibles antes de entrenar.     |
| **Reducci√≥n de overfitting**  | T√©cnicas como dropout, data augmentation, etc., para mejorar la generalizaci√≥n.  |

## üß™ Ejemplos pr√°cticos

| T√©cnica               | Ejemplo                                                                 |
|----------------------|-------------------------------------------------------------------------|
| Regularizaci√≥n        | A√±adir L2 al loss: `loss += Œª * ||weights||¬≤`                           |
| Early Stopping        | Detener entrenamiento si no mejora la validaci√≥n en 5 √©pocas seguidas   |
| Differential Privacy | Entrenar con `dp-SGD` (gradient descent con privacidad diferencial)     |
| Dataset Sanitization | Eliminar emails, tel√©fonos o textos confidenciales antes de entrenar     |
| Dropout               | Desactivar aleatoriamente neuronas durante el entrenamiento (p=0.5)     |

## ‚ö†Ô∏è Casos reales de filtraci√≥n o memorizaci√≥n de datos

| Caso                                 | Descripci√≥n                                                                                       | A√±o  |
|--------------------------------------|---------------------------------------------------------------------------------------------------|------|
| **GPT-2 / GPT-3 (OpenAI)**           | Algunos modelos generaban fragmentos textuales con datos personales como n√∫meros de tel√©fono.     | 2020 |
| **Codenames leak (GitHub Copilot)**  | Copilot generaba fragmentos de c√≥digo con claves API o contrase√±as extra√≠das del set de entrenamiento. | 2021 |
| **Stable Diffusion**                 | Algunos modelos de texto a imagen pod√≠an reproducir rostros reales de personas de los datasets.   | 2022 |
| **Imagenet Faces**                   | Dataset conten√≠a im√°genes de personas sin consentimiento; usarse en modelos generaba problemas legales. | 2019 |
| **Memorization in Language Models**  | Estudio de Google y Stanford demostr√≥ que modelos LLM pueden memorizar y reproducir datos de entrenamiento al pie de la letra. | 2021 |

## üîê ¬øC√≥mo protege ChatGPT los *prompts*?

| Mecanismo                         | Descripci√≥n breve                                                                  |
|----------------------------------|-------------------------------------------------------------------------------------|
| **Cifrado en tr√°nsito (TLS)**    | Los prompts se transmiten cifrados desde tu dispositivo hasta los servidores.      |
| **Acceso controlado**            | Solo personal autorizado puede acceder a datos internos, bajo estrictas pol√≠ticas. |
| **Almacenamiento seguro**        | Los datos se almacenan en sistemas protegidos con cifrado y control de acceso.     |
| **Reducci√≥n de retenci√≥n**       | En algunos modos, los datos no se guardan o se eliminan tras cierto tiempo.        |
| **No entrenamiento con tus datos (por defecto)** | ChatGPT no entrena con tus datos a menos que des consentimiento expl√≠cito.        |
| **Auditor√≠a y cumplimiento**     | Se aplican controles para cumplir con normas como GDPR o ISO 27001.                |

### üßæ Ejemplo: ¬øSe guardan tus prompts?

| Caso de uso              | ¬øSe guarda el prompt?         | ¬øSe usa para entrenamiento? |
|--------------------------|-------------------------------|------------------------------|
| Usuario no autenticado   | Temporalmente, para funcionamiento | No                          |
| Usuario autenticado, sin consentimiento | Solo para mejorar la experiencia | No                    |
| Usuario autenticado, con consentimiento | S√≠, con anonimizaci√≥n y para mejorar el modelo | S√≠         |
